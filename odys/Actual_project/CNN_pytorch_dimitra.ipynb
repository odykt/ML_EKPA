{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62aae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 images.\n",
      "Image shape: (64, 64) \n",
      "Labels: ['AbdomenCT' 'BreastMRI' 'Hand' 'HeadCT']\n"
     ]
    }
   ],
   "source": [
    "# Load images and labels\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the input directory\n",
    "input_dir = 'C:/Users/odys_/Desktop/ML_winows_2/archive_small'\n",
    "\n",
    "# Initialize lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each folder (class name) in the input directory\n",
    "for category in os.listdir(input_dir):\n",
    "    category_path = os.path.join(input_dir, category)\n",
    "    if os.path.isdir(category_path):  # Ensure it's a directory\n",
    "        for file_name in os.listdir(category_path):\n",
    "            if file_name.endswith('.jpeg') or file_name.endswith('.png'):  # Check for valid image files\n",
    "                img_path = os.path.join(category_path, file_name)\n",
    "                try:\n",
    "                    # Open and preprocess the image\n",
    "                    img = Image.open(img_path)\n",
    "                    # img = img.resize((128, 128))  # Resize to 128x128\n",
    "                    img_array = np.array(img)\n",
    "                    \n",
    "                    # Append the image and its label\n",
    "                    images.append(img_array)\n",
    "                    labels.append(category)  # Use the folder name as the label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print some information about the loaded data\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(f\"Image shape: {images[0].shape} \")\n",
    "print(f\"Labels: {np.unique(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3834da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Train set: (420, 64, 64), labels: (420,)\n",
      "Shape Test set: (180, 64, 64), labels:(180,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random_state for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "# Split into train and test only\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=random_state, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Shape Train set: {X_train.shape}, labels: {y_train.shape}\")\n",
    "print(f\"Shape Test set: {X_test.shape}, labels:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4ba811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 8.8333, Accuracy: 68.33%\n",
      "Epoch 2/20, Loss: 0.0199, Accuracy: 99.29%\n",
      "Epoch 3/20, Loss: 0.0014, Accuracy: 100.00%\n",
      "Epoch 4/20, Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch 5/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 6/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 7/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 8/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 9/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 10/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 11/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 12/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 13/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 14/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 15/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 16/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 17/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 18/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 19/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch 20/20, Loss: 0.0000, Accuracy: 100.00%\n",
      "Test Accuracy: 99.44%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Μετατροπή ετικετών σε αριθμούς\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Μετατροπή των εικόνων σε PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Προσθήκη καναλιού (1 για grayscale)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Δημιουργία TensorDataset και DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "test_data = TensorDataset(X_test_tensor, torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Ορισμός του CNN μοντέλου\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1: κανάλι εικόνας (grayscale)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Προσαρμογή ανάλογα με το μέγεθος των εικόνων\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten το tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Ορισμός του μοντέλου, κριτηρίου και optimizer\n",
    "model = CNN(num_classes=len(np.unique(y_train)))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Εκπαίδευση του μοντέλου\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "# Αξιολόγηση του μοντέλου στο Test Set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c624316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\odys_\\Desktop\\ML_winows_2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-01 23:51:16,286] A new study created in memory with name: no-name-3f9b5556-0e80-4a17-aad2-03ad7a5ecba5\n",
      "C:\\Users\\odys_\\AppData\\Local\\Temp\\ipykernel_10088\\1918868498.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
      "C:\\Users\\odys_\\AppData\\Local\\Temp\\ipykernel_10088\\1918868498.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout', 0.2, 0.5)\n",
      "[I 2025-06-01 23:52:28,500] Trial 0 finished with value: -99.44444444444444 and parameters: {'lr': 0.0006488487548052437, 'batch_size': 64, 'n_filters1': 64, 'n_filters2': 64, 'n_filters3': 128, 'fc_size': 512, 'dropout': 0.21010390781363764}. Best is trial 0 with value: -99.44444444444444.\n",
      "[I 2025-06-01 23:53:32,968] Trial 1 finished with value: -97.22222222222223 and parameters: {'lr': 0.0013914606644257297, 'batch_size': 64, 'n_filters1': 64, 'n_filters2': 64, 'n_filters3': 128, 'fc_size': 128, 'dropout': 0.2702861390068038}. Best is trial 0 with value: -99.44444444444444.\n",
      "[I 2025-06-01 23:53:53,642] Trial 2 finished with value: -100.0 and parameters: {'lr': 0.00026217275565833665, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 32, 'n_filters3': 64, 'fc_size': 512, 'dropout': 0.45944441481938225}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:54:51,446] Trial 3 finished with value: -100.0 and parameters: {'lr': 0.007406872806218168, 'batch_size': 64, 'n_filters1': 32, 'n_filters2': 128, 'n_filters3': 128, 'fc_size': 256, 'dropout': 0.4518607385346476}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:55:21,258] Trial 4 finished with value: -98.88888888888889 and parameters: {'lr': 0.00014679692664652534, 'batch_size': 32, 'n_filters1': 32, 'n_filters2': 32, 'n_filters3': 256, 'fc_size': 128, 'dropout': 0.48899850119418103}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:55:54,151] Trial 5 finished with value: -100.0 and parameters: {'lr': 0.006174185875940533, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 64, 'n_filters3': 64, 'fc_size': 512, 'dropout': 0.4018197763234896}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:56:47,111] Trial 6 finished with value: -91.66666666666667 and parameters: {'lr': 0.0032623291071563277, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 64, 'n_filters3': 256, 'fc_size': 512, 'dropout': 0.2814179082188963}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:57:15,256] Trial 7 finished with value: -25.0 and parameters: {'lr': 0.009761949624962022, 'batch_size': 16, 'n_filters1': 64, 'n_filters2': 32, 'n_filters3': 128, 'fc_size': 128, 'dropout': 0.22277650732472554}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:57:38,612] Trial 8 finished with value: -99.44444444444444 and parameters: {'lr': 0.001138310325282985, 'batch_size': 64, 'n_filters1': 32, 'n_filters2': 32, 'n_filters3': 128, 'fc_size': 256, 'dropout': 0.27465736040210464}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:58:32,859] Trial 9 finished with value: -98.33333333333333 and parameters: {'lr': 0.0004620407498506156, 'batch_size': 64, 'n_filters1': 64, 'n_filters2': 32, 'n_filters3': 256, 'fc_size': 512, 'dropout': 0.2890321053089253}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-01 23:59:11,312] Trial 10 finished with value: -100.0 and parameters: {'lr': 0.00011032856760350577, 'batch_size': 32, 'n_filters1': 16, 'n_filters2': 128, 'n_filters3': 64, 'fc_size': 512, 'dropout': 0.36987170666400326}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:00:21,624] Trial 11 finished with value: -100.0 and parameters: {'lr': 0.00026125798374362224, 'batch_size': 16, 'n_filters1': 32, 'n_filters2': 128, 'n_filters3': 64, 'fc_size': 256, 'dropout': 0.4724807974988385}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:01:16,863] Trial 12 finished with value: -98.88888888888889 and parameters: {'lr': 0.0023822308694956664, 'batch_size': 64, 'n_filters1': 32, 'n_filters2': 128, 'n_filters3': 64, 'fc_size': 256, 'dropout': 0.4372492397889216}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:02:08,816] Trial 13 finished with value: -99.44444444444444 and parameters: {'lr': 0.00027037412129481966, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 128, 'n_filters3': 128, 'fc_size': 256, 'dropout': 0.43308553527897836}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:02:29,088] Trial 14 finished with value: -95.0 and parameters: {'lr': 0.0032557632766938654, 'batch_size': 32, 'n_filters1': 32, 'n_filters2': 32, 'n_filters3': 64, 'fc_size': 256, 'dropout': 0.35828100416084746}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:03:45,929] Trial 15 finished with value: -97.77777777777777 and parameters: {'lr': 0.0005944325932171079, 'batch_size': 64, 'n_filters1': 16, 'n_filters2': 128, 'n_filters3': 128, 'fc_size': 512, 'dropout': 0.4511456779269448}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:04:33,916] Trial 16 finished with value: -99.44444444444444 and parameters: {'lr': 0.00021606393404125803, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 128, 'n_filters3': 64, 'fc_size': 256, 'dropout': 0.389122274987705}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:05:02,278] Trial 17 finished with value: -97.77777777777777 and parameters: {'lr': 0.001749747601213144, 'batch_size': 64, 'n_filters1': 32, 'n_filters2': 32, 'n_filters3': 128, 'fc_size': 512, 'dropout': 0.4993841170393959}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:06:13,162] Trial 18 finished with value: -98.88888888888889 and parameters: {'lr': 0.004964093742313803, 'batch_size': 16, 'n_filters1': 32, 'n_filters2': 128, 'n_filters3': 64, 'fc_size': 256, 'dropout': 0.3258992345814642}. Best is trial 2 with value: -100.0.\n",
      "[I 2025-06-02 00:07:01,046] Trial 19 finished with value: -98.88888888888889 and parameters: {'lr': 0.0003958383631723853, 'batch_size': 32, 'n_filters1': 16, 'n_filters2': 32, 'n_filters3': 256, 'fc_size': 128, 'dropout': 0.4134772790851257}. Best is trial 2 with value: -100.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00026217275565833665, 'batch_size': 16, 'n_filters1': 16, 'n_filters2': 32, 'n_filters3': 64, 'fc_size': 512, 'dropout': 0.45944441481938225}\n",
      "Best test accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    n_filters1 = trial.suggest_categorical('n_filters1', [16, 32, 64])\n",
    "    n_filters2 = trial.suggest_categorical('n_filters2', [32, 64, 128])\n",
    "    n_filters3 = trial.suggest_categorical('n_filters3', [64, 128, 256])\n",
    "    fc_size = trial.suggest_categorical('fc_size', [128, 256, 512])\n",
    "    dropout_rate = trial.suggest_uniform('dropout', 0.2, 0.5)\n",
    "\n",
    "    # DataLoader with new batch size\n",
    "    train_data = TensorDataset(X_train_tensor, torch.tensor(y_train_encoded, dtype=torch.long))\n",
    "    test_data = TensorDataset(X_test_tensor, torch.tensor(y_test_encoded, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define model with trial hyperparameters\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, n_filters1, kernel_size=3, padding=1)\n",
    "            self.conv2 = nn.Conv2d(n_filters1, n_filters2, kernel_size=3, padding=1)\n",
    "            self.conv3 = nn.Conv2d(n_filters2, n_filters3, kernel_size=3, padding=1)\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            # Calculate the size after convolutions and pooling\n",
    "            self._to_linear = n_filters3 * 8 * 8  # Adjust if your image size is not 64x64\n",
    "            self.fc1 = nn.Linear(self._to_linear, fc_size)\n",
    "            self.fc2 = nn.Linear(fc_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.conv1(x))\n",
    "            x = self.maxpool(x)\n",
    "            x = self.relu(self.conv2(x))\n",
    "            x = self.maxpool(x)\n",
    "            x = self.relu(self.conv3(x))\n",
    "            x = self.maxpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(self.relu(self.fc1(x)))\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    model = CNN(num_classes=len(np.unique(y_train)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop (fewer epochs for speed)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    return -test_accuracy  # Optuna minimizes, so return negative accuracy\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best test accuracy: {:.2f}%\".format(-study.best_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
